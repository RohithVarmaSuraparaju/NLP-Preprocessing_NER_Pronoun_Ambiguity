{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3WNhfioWkMX",
        "outputId": "99b46faf-7f6e-465f-f286-1d7745a6439f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Install required NLP libraries in Google Colab\n",
        "!pip install nltk spacy\n",
        "\n",
        "# Download spaCy English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Download NLTK resources\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QAhBx90JZe8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "# Required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith(\"N\"):\n",
        "        return \"n\"\n",
        "    if tag.startswith(\"V\"):\n",
        "        return \"v\"\n",
        "    return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    print(\"===== Q1 Output Steps =====\")\n",
        "\n",
        "    # 1. SEGMENT INTO TOKENS\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"1. Segmented Tokens:\")\n",
        "    print(tokens, \"\\n\")\n",
        "\n",
        "    # 2. REMOVE STOPWORDS\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered = [w for w in tokens if w.lower() not in stop_words]\n",
        "    print(\"2. After Stopword Removal:\")\n",
        "    print(filtered, \"\\n\")\n",
        "\n",
        "    # 3. LEMMATIZATION (all tokens after stopwords)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(w.lower()) for w in filtered]\n",
        "    print(\"3. Lemmatized Tokens (before POS filtering):\")\n",
        "    print(lemmas, \"\\n\")\n",
        "\n",
        "    # 4. KEEP ONLY VERBS & NOUNS (with proper POS-based lemmatization)\n",
        "    pos_tags = pos_tag(filtered)\n",
        "    final_output = []\n",
        "\n",
        "    for word, tag in pos_tags:\n",
        "        wn_tag = get_wordnet_pos(tag)\n",
        "        if wn_tag in (\"n\", \"v\"):\n",
        "            lemma = lemmatizer.lemmatize(word.lower(), wn_tag)\n",
        "            final_output.append(lemma)\n",
        "\n",
        "    print(\"4. Final Output (Only Verbs & Nouns, Lemmatized):\")\n",
        "    print(final_output)\n",
        "\n",
        "    return tokens, filtered, lemmas, final_output\n",
        "\n",
        "\n",
        "# INPUT TEXT\n",
        "text_q1 = \"John enjoys playing football while Mary loves reading books in the library.\"\n",
        "\n",
        "# RUN PIPELINE\n",
        "tokens, filtered, lemmas, final_output = preprocess_text(text_q1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNAEx2KCX8A7",
        "outputId": "33269ce7-c216-4e2f-de7c-88996f573777"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Q1 Output Steps =====\n",
            "1. Segmented Tokens:\n",
            "['John', 'enjoys', 'playing', 'football', 'while', 'Mary', 'loves', 'reading', 'books', 'in', 'the', 'library', '.'] \n",
            "\n",
            "2. After Stopword Removal:\n",
            "['John', 'enjoys', 'playing', 'football', 'Mary', 'loves', 'reading', 'books', 'library', '.'] \n",
            "\n",
            "3. Lemmatized Tokens (before POS filtering):\n",
            "['john', 'enjoys', 'playing', 'football', 'mary', 'love', 'reading', 'book', 'library', '.'] \n",
            "\n",
            "4. Final Output (Only Verbs & Nouns, Lemmatized):\n",
            "['john', 'enjoy', 'play', 'football', 'mary', 'love', 'read', 'book']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q2: Named Entity Recognition (NER) + Pronoun ambiguity warning\n",
        "\n",
        "Tasks:\n",
        "1. Perform Named Entity Recognition (NER)\n",
        "2. If the text contains a pronoun (\"he\", \"she\", \"they\", \"him\", \"her\", \"them\"),\n",
        "   print: \"Warning: Possible pronoun ambiguity detected!\"\n",
        "\n",
        "Input text:\n",
        "\"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\"\n",
        "\"\"\"\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def analyze_text(text: str):\n",
        "    \"\"\"\n",
        "    Run NER and check for pronoun ambiguity in the given text.\n",
        "    Prints:\n",
        "    - all named entities\n",
        "    - warning if ambiguous pronouns are present\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # 1. Named Entity Recognition (NER)\n",
        "    print(\"===== Q2 Output =====\")\n",
        "    print(\"1. Named Entities:\\n\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"  {ent.text:20s} -> {ent.label_}\")\n",
        "\n",
        "    # 2. Pronoun ambiguity detection\n",
        "    pronouns = {\"he\", \"she\", \"they\", \"him\", \"her\", \"them\"}\n",
        "    tokens_lower = [token.text.lower() for token in doc]\n",
        "\n",
        "    print(\"\\n2. Pronoun Ambiguity Check:\")\n",
        "    print(\"Tokens:\", tokens_lower)\n",
        "\n",
        "    if any(p in tokens_lower for p in pronouns):\n",
        "        print(\"\\nWarning: Possible pronoun ambiguity detected!\")\n",
        "    else:\n",
        "        print(\"\\nNo ambiguous pronouns detected.\")\n",
        "\n",
        "# Input text for Q2\n",
        "text_q2 = \"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\"\n",
        "\n",
        "# Run analysis\n",
        "analyze_text(text_q2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppZxk_6tZrS4",
        "outputId": "092133e8-cbc6-46d3-daac-2aebb46e00ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Q2 Output =====\n",
            "1. Named Entities:\n",
            "\n",
            "  Chris                -> PERSON\n",
            "  Alex                 -> PERSON\n",
            "  Apple                -> ORG\n",
            "  California           -> GPE\n",
            "  iPhone               -> ORG\n",
            "\n",
            "2. Pronoun Ambiguity Check:\n",
            "Tokens: ['chris', 'met', 'alex', 'at', 'apple', 'headquarters', 'in', 'california', '.', 'he', 'told', 'him', 'about', 'the', 'new', 'iphone', 'launch', '.']\n",
            "\n",
            "Warning: Possible pronoun ambiguity detected!\n"
          ]
        }
      ]
    }
  ]
}